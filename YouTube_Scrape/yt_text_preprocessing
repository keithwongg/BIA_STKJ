# Import libraries
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re
import string

# Input string
def preProcessing(words):
    
    # Change all to lowercase
    words = words.lower()
    
    # Remove punctuation
    translator = re.compile('[%s]' % re.escape(string.punctuation))
    words = translator.sub(' ', words)
    
    # Remove URLs
    words = re.sub(r"http\S+", "", words)
    
    # Tokenize
    words = word_tokenize(words)

    # Lemmatize
    lemmatizer = WordNetLemmatizer()
    new_words = []
    for i in words:
        new_words.append(lemmatizer.lemmatize(i))

    return new_words
